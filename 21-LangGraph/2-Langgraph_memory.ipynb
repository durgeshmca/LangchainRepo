{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add semantic search to your agent's memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6917/2741872573.py:4: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\n",
      "  embedding = init_embeddings(model=\"all-MiniLM-L6-v2\",provider=\"huggingface\")\n",
      "/official/official_docs/PythonLearning/GenerativeAIWithLangchainByKrish/projects/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "\n",
    "embedding = init_embeddings(model=\"all-MiniLM-L6-v2\",provider=\"huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory: I prefer Italian food (similarity: 0.4203020835282931)\n",
      "Memory: I love pizza (similarity: 0.3255428269829057)\n",
      "Memory: I am a plumber (similarity: 0.13404835760593414)\n"
     ]
    }
   ],
   "source": [
    "PG1_CONNECTION = \"postgresql://unicode:unicode@localhost:5432/pa\"\n",
    "# connection_kwargs = {\n",
    "#     \"autocommit\": True,\n",
    "#     \"prepare_threshold\": 0,\n",
    "# }\n",
    "# from psycopg import Connection\n",
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store2:\n",
    "    # store.delete((\"user_123\", \"memories\"), \"1\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"2\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    # store.delete((\"user_123\", \"memories\"), \"3\")\n",
    "    store2.setup()#only once for migration\n",
    "    # Store some memories\n",
    "    store2.put((\"user_124\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"2\", {\"text\": \"I prefer Italian food\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I don't like spicy food\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I am studying econometrics\"})\n",
    "    store2.put((\"user_124\", \"memories\"), \"3\", {\"text\": \"I am a plumber\"})\n",
    "\n",
    "    memories = store2.search((\"user_124\", \"memories\"), query=\"I like food?\", limit=5)\n",
    "\n",
    "    for memory in memories:\n",
    "        print(f'Memory: {memory.value[\"text\"]} (similarity: {memory.score})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using in your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import START,StateGraph,MessagesState\n",
    "\n",
    "llm = init_chat_model(model=\"llama-3.3-70b-versatile\",model_provider=\"groq\")\n",
    "\n",
    "def chat(state, *, store: BaseStore):\n",
    "    # Search based on user's last message\n",
    "    print(state)\n",
    "    items = store.search(\n",
    "        (\"user_124\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n",
    "    )\n",
    "    print(items)\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='I am hungry', additional_kwargs={}, response_metadata={}, id='b0833885-1b97-412d-8bcc-480d34e5d0d4')]}\n",
      "[Item(namespace=['user_124', 'memories'], key='2', value={'text': 'I prefer Italian food'}, created_at='2025-03-18T17:33:39.745910+05:30', updated_at='2025-03-18T17:33:39.745910+05:30', score=0.3394367503448451), Item(namespace=['user_124', 'memories'], key='1', value={'text': 'I love pizza'}, created_at='2025-03-18T17:33:39.725608+05:30', updated_at='2025-03-18T17:33:39.725608+05:30', score=0.300432819153835)]\n",
      "You must be craving something delicious. How about some Italian food? Maybe a nice, hot pizza? Would you like me to suggest some pizza toppings or recommend a nearby Italian restaurant?"
     ]
    }
   ],
   "source": [
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store :\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(chat)\n",
    "    builder.add_edge(START,\"chat\")\n",
    "    graph = builder.compile(store=store)\n",
    "    for message, metadata in graph.stream(\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": \"I am hungry\"}]},\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using in create_react_agent\n",
    "#### Add semantic search to your tool calling agent by injecting the store in the prompt function. You can also use the store in a tool to let your agent manually store or search for memories.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored memory f1ec916f-e895-4c8f-9323-327d396d947aStored memory 92e83d0d-f89d-438b-a04e-4e4da3e35b7dIt seems like you're hungry! How about making or ordering a pizza? What toppings would you like on it? Let me know if you need any recommendations!"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import  BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def prepare_messages(state: StateGraph,*, store:BaseStore):\n",
    "    # search based on user's last message\n",
    "\n",
    "    items = store.search(\n",
    "        (\"user_124\", \"memories\"),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "        limit=2\n",
    "        )\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"}\n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "# You can also use the store directly within a tool!\n",
    "def upsert_memory(\n",
    "    content: str,\n",
    "    *,\n",
    "    memory_id: Optional[uuid.UUID] = None,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    "):\n",
    "    \"\"\"Upsert a memory in the database.\"\"\"\n",
    "    # The LLM can use this tool to store a new memory\n",
    "    mem_id = memory_id or uuid.uuid4()\n",
    "    store.put(\n",
    "        (\"user_124\", \"memories\"),\n",
    "        key=str(mem_id),\n",
    "        value={\"text\": content},\n",
    "    )\n",
    "    return f\"Stored memory {mem_id}\"\n",
    "\n",
    "# create agent\n",
    "with PostgresStore.from_conn_string(\n",
    "    conn_string=PG1_CONNECTION,index={\n",
    "        \"embed\": embedding,\n",
    "        \"dims\": 384,\n",
    "        \"fields\": [\"text\"]  # specify which fields to embed. Default is the whole serialized valu\n",
    "    }\n",
    ") as store :\n",
    "    agent = create_react_agent(\n",
    "        init_chat_model(model=\"deepseek-r1-distill-llama-70b\",model_provider=\"groq\"),\n",
    "        tools=[upsert_memory],\n",
    "        # The 'prompt' function is run to prepare the messages for the LLM. It is called\n",
    "        # right before each LLM call\n",
    "        prompt=prepare_messages,\n",
    "        store=store,\n",
    "    )\n",
    "    for message, metadata in agent.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        print(message.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Usage\n",
    "#### Multi-vector indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://langchain-ai.github.io/langgraph/how-tos/memory/semantic-search/#advanced-usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to add cross-thread persistence to your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
